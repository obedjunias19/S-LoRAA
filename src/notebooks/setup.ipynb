{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c2102",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089b2b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e5302",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "workspace = '/content/drive/MyDrive/S-LoRAA'\n",
    "os.makedirs(workspace, exist_ok=True)\n",
    "os.chdir(workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817af40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('S-LoRAA'):\n",
    "    !git clone https://github.com/obedjunias19/S-LoRAA.git\n",
    "    os.chdir('S-LoRAA')\n",
    "else:\n",
    "    os.chdir('LoRAA')\n",
    "    !git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff13394",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q vllm\n",
    "!pip install -q transformers==4.53.0 peft\n",
    "!pip install -q accelerate huggingface-hub\n",
    "!pip install -q pandas matplotlib seaborn networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340df25b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"  CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67711c8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Use Drive for caching (persistent across sessions)\n",
    "cache_dir = f\"{workspace}/model_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "print(\"Downloading models...\")\n",
    "\n",
    "# Base model\n",
    "base_model_path = snapshot_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b-hf\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "print(f\"Base model: {base_model_path}\")\n",
    "\n",
    "# SQL LoRA\n",
    "sql_lora_path = snapshot_download(\n",
    "    repo_id=\"yard1/llama-2-7b-sql-lora-test\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "print(f\"SQL LoRA: {sql_lora_path}\")\n",
    "\n",
    "# Code LoRA\n",
    "code_lora_path = snapshot_download(\n",
    "    repo_id=\"monsterapi/llama2-code-generation\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "print(f\"Code LoRA: {code_lora_path}\")\n",
    "\n",
    "# Save paths for later\n",
    "with open('model_paths.txt', 'w') as f:\n",
    "    f.write(f\"BASE_MODEL={base_model_path}\\n\")\n",
    "    f.write(f\"SQL_LORA={sql_lora_path}\\n\")\n",
    "    f.write(f\"CODE_LORA={code_lora_path}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "print(\"Testing vLLM...\")\n",
    "\n",
    "# Initialize (smaller memory usage for Colab)\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.8,  \n",
    "    max_model_len=1024\n",
    ")\n",
    "\n",
    "# Test generation\n",
    "prompts = [\"The capital of France is\", \"Python is a programming\"]\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=20)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"\\nTest Outputs:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{i+1}. {output.outputs[0].text}\")\n",
    "\n",
    "print(\"\\nvLLM working!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
