{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c2102",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089b2b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e5302",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/content/drive/MyDrive\")\n",
    "\n",
    "repo_url = \"https://github.com/obedjunias19/S-LoRAA.git\"\n",
    "workspace = \"/content/drive/MyDrive/S-LoRAA\"\n",
    "\n",
    "# If repo doesn't exist or wasn't fully cloned\n",
    "if not os.path.exists(os.path.join(workspace, \".git\")):\n",
    "    # Remove any partial folder\n",
    "    !rm -rf {workspace}\n",
    "    # Fresh clone\n",
    "    !git clone {repo_url} {workspace}\n",
    "\n",
    "# Enter repo\n",
    "os.chdir(workspace)\n",
    "\n",
    "# Update repo\n",
    "!git pull origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff13394",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q vllm\n",
    "!pip install -q transformers==4.53.0 peft\n",
    "!pip install -q accelerate huggingface-hub\n",
    "!pip install -q pandas matplotlib seaborn networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340df25b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"  CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67711c8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Use Drive for caching (persistent across sessions)\n",
    "cache_dir = f\"{workspace}/model_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "print(\"Downloading models...\")\n",
    "\n",
    "# Base model\n",
    "base_model_path = snapshot_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b-hf\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "print(f\"Base model: {base_model_path}\")\n",
    "\n",
    "# SQL LoRA\n",
    "sql_lora_path = snapshot_download(\n",
    "    repo_id=\"yard1/llama-2-7b-sql-lora-test\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "print(f\"SQL LoRA: {sql_lora_path}\")\n",
    "\n",
    "# Code LoRA\n",
    "code_lora_path = snapshot_download(\n",
    "    repo_id=\"monsterapi/llama2-code-generation\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "print(f\"Code LoRA: {code_lora_path}\")\n",
    "\n",
    "# Save paths for later\n",
    "with open('model_paths.txt', 'w') as f:\n",
    "    f.write(f\"BASE_MODEL={base_model_path}\\n\")\n",
    "    f.write(f\"SQL_LORA={sql_lora_path}\\n\")\n",
    "    f.write(f\"CODE_LORA={code_lora_path}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "print(\"Testing vLLM...\")\n",
    "\n",
    "# Initialize (smaller memory usage for Colab)\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.8,  \n",
    "    max_model_len=1024\n",
    ")\n",
    "\n",
    "# Test generation\n",
    "prompts = [\"The capital of France is\", \"Python is a programming\"]\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=20)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"\\nTest Outputs:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{i+1}. {output.outputs[0].text}\")\n",
    "\n",
    "print(\"\\nvLLM working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514abb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/S-LoRAA/src')\n",
    "\n",
    "\n",
    "from core.vllm_backend import VLLMBackend\n",
    "\n",
    "\n",
    "print(\"Testing VLLMBackend...\")\n",
    "\n",
    "# Initialize\n",
    "backend = VLLMBackend(\n",
    "    model_path=\"meta-llama/Llama-2-7b-hf\",\n",
    "    max_loras=4,\n",
    "    gpu_memory_utilization=0.9\n",
    ")\n",
    "\n",
    "# Test 1: Base model\n",
    "print(\"\\n Testing base model generation...\")\n",
    "outputs = backend.generate(\n",
    "    prompts=[\"The capital of France is\", \"Python is\"],\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"   Output {i+1}: {out}\")\n",
    "\n",
    "# Test 2: With SQL LoRA\n",
    "print(\"\\nTesting with SQL LoRA...\")\n",
    "sql_outputs = backend.generate(\n",
    "    prompts=[\"SELECT * FROM users WHERE age > 25\"],\n",
    "    lora_path=sql_lora_path,\n",
    "    lora_id=1,\n",
    "    temperature=0.1,\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f\"   SQL Output: {sql_outputs[0]}\")\n",
    "\n",
    "# Test 3: Health check\n",
    "print(\"\\nHealth check...\")\n",
    "health = backend.health_check()\n",
    "print(f\"   Status: {'OK' if health else 'Failed'}\")\n",
    "\n",
    "print(\"\\nAll tests passed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
